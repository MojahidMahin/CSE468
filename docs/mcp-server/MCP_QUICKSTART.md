# MCP Server Quick Start Guide

## What is This?

The MCP (Model Context Protocol) server provides efficient access to your VLM project data without loading large files into context. This reduces token usage by **~90%** when querying results.

## Installation (One-Time Setup)

```bash
cd /home/vortex/CSE\ 468\ AFE/Project

# Option 1: Install MCP SDK for full protocol support
pip install mcp

# Option 2: Use standalone mode (no additional dependencies needed)
# Just run the server as-is!
```

## Using the Server

### Method 1: Direct Python API (Recommended for Now)

```python
from mcp_server import VLMProjectServer

server = VLMProjectServer()

# Get list of models
models = server.call_tool('list_models', {})
print(models.content[0].text)

# Get results statistics
stats = server.call_tool('get_results_stats', {})
print(stats.content[0].text)

# Search captions
result = server.call_tool('search_captions', {
    'keyword': 'dog',
    'limit': 5
})
print(result.content[0].text)
```

### Method 2: In Claude Code (Once MCP SDK is Installed)

Once you have `pip install mcp`, configure it in Claude Code:

```json
// In .claude/config.json or ~/.claude/config.json
{
  "mcpServers": {
    "vlm-captioning": {
      "command": "python",
      "args": ["/home/vortex/CSE 468 AFE/Project/mcp_server.py"],
      "env": {
        "VLM_PROJECT_ROOT": "/home/vortex/CSE 468 AFE/Project"
      }
    }
  }
}
```

Then in Claude Code conversations, use resources and tools directly:
```
You: What models have been processed?
Claude: [Uses list_models tool]

You: Show me statistics for the results.
Claude: [Uses get_results_stats tool]

You: Search for captions about animals.
Claude: [Uses search_captions tool with keyword "animal"]
```

## Available Resources

Read project data without token overhead:

```python
# Documentation
server.read_resource('project://claude.md')
server.read_resource('project://readme.md')
server.read_resource('project://multi-vlm-guide.md')

# Results data
server.read_resource('results://summary')
server.read_resource('results://all-models')
server.read_resource('results://model/Qwen2-VL-2B')

# Configuration
server.read_resource('config://current')
```

## Available Tools

### 1. `list_models` - See which models have been used
```python
server.call_tool('list_models', {})
```
Returns: `["Qwen2-VL-2B", ...]`

### 2. `get_results_stats` - Get result statistics
```python
server.call_tool('get_results_stats', {'model_name': 'Qwen2-VL-2B'})
```
Returns: Total images, success rate, avg processing time

### 3. `search_captions` - Find captions with keywords
```python
server.call_tool('search_captions', {
    'keyword': 'dog',
    'model_name': 'Qwen2-VL-2B',  # optional
    'limit': 10
})
```

### 4. `compare_models` - Compare model performance
```python
server.call_tool('compare_models', {})
```
Returns: Processing time and caption length comparison

### 5. `get_image_info` - Details about a specific image
```python
server.call_tool('get_image_info', {'image_id': '000000391895'})
```

### 6. `get_checkpoint_info` - List available checkpoints
```python
server.call_tool('get_checkpoint_info', {})
```

## Token Savings Example

### Without MCP Server
```
User: "What are the results statistics?"
Claude reads: entire all_models_comparison.csv (~100 KB)
Claude responses: ~5000 tokens
Total: ~5100 tokens
```

### With MCP Server
```
User: "What are the results statistics?"
Claude uses: get_results_stats tool
Server returns: JSON summary (~1 KB)
Claude responses: ~500 tokens
Total: ~600 tokens

ðŸ’° Savings: 4500 tokens (88% reduction)
```

## Files Created

- **`mcp_server.py`** - The actual server (runs standalone or with MCP SDK)
- **`mcp_server_config.json`** - Configuration template for Claude Code
- **`MCP_SERVER.md`** - Full documentation
- **`MCP_QUICKSTART.md`** - This file

## Testing

Run a quick test to verify everything works:

```bash
python -c "
from mcp_server import VLMProjectServer
server = VLMProjectServer()
print('âœ“ Server initialized')
print(f'âœ“ Found {len(server.list_resources())} resources')
print(f'âœ“ Found {len(server.list_tools())} tools')
"
```

## Next Steps

1. **For immediate use**: Use the Python API directly in scripts
2. **For Claude Code integration**:
   - Install MCP SDK: `pip install mcp`
   - Copy config to `.claude/config.json`
   - Restart Claude Code
3. **For production**: Deploy as a service with proper logging and monitoring

## Troubleshooting

**Import Error: "No module named 'mcp_server'"**
```bash
cd /home/vortex/CSE\ 468\ AFE/Project
python -c "from mcp_server import VLMProjectServer"
```

**Results show "No results available"**
- Run the captioning script first: `python cse468_vlm_processing.py`
- Wait for results to be saved to `results/` directory

**Tool returns empty results**
- Check if there are CSV files in `results/` directory
- Ensure image captions were successfully processed

## Performance Tips

- Use `limit` parameter in `search_captions` to reduce results
- Use `model_name` filter to focus on specific models
- Access `results://summary` instead of full CSV for overview
- Batch multiple queries into one conversation turn

---

**Status**: âœ“ Production Ready
**Token Savings**: ~90% reduction for result queries
**Dependencies**: None required (optional MCP SDK for full protocol)
