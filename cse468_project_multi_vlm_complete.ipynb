{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-VLM Comprehensive Comparison Framework\n",
    "## Testing 7 Vision-Language Models on COCO Dataset\n",
    "Optimized for RTX 5080 (16GB VRAM) with all models fully implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only needed for Google Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# Install all required packages\n",
    "!pip install transformers torch torchvision Pillow datasets pycocotools\n",
    "!pip install -q accelerate bitsandbytes qwen-vl-utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import requests\n",
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch\n",
    "import gc\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "ANNOTATIONS_PATH = '/home/vortex/CSE 468 AFE/Project/annotations'\n",
    "IMAGES_DIR = 'coco_images'\n",
    "RESULTS_DIR = 'results'\n",
    "NUM_IMAGES = 200\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(IMAGES_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Setup complete.\")\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load COCO Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations from local copy\n",
    "print(\"Loading COCO annotations...\")\n",
    "coco = COCO(os.path.join(ANNOTATIONS_PATH, 'captions_val2017.json'))\n",
    "\n",
    "# Select random images\n",
    "import random\n",
    "random.seed(42)\n",
    "all_img_ids = coco.getImgIds()\n",
    "selected_img_ids = random.sample(all_img_ids, NUM_IMAGES)\n",
    "\n",
    "print(f\"Selected {len(selected_img_ids)} images\")\n",
    "\n",
    "# Prepare images\n",
    "image_files = []\n",
    "for img_id in tqdm(selected_img_ids, desc=\"Checking images\"):\n",
    "    img_info = coco.loadImgs(img_id)[0]\n",
    "    img_path = os.path.join(IMAGES_DIR, img_info['file_name'])\n",
    "    \n",
    "    if not os.path.exists(img_path):\n",
    "        try:\n",
    "            img_url = img_info['coco_url']\n",
    "            img_data = requests.get(img_url).content\n",
    "            with open(img_path, 'wb') as f:\n",
    "                f.write(img_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {img_info['file_name']}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    image_files.append(img_info['file_name'])\n",
    "\n",
    "print(f\"Ready with {len(image_files)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VLM Model 1: Qwen2-VL-2B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "class QwenVLM:\n",
    "    \"\"\"Qwen2-VL-2B - Alibaba's efficient vision-language model\n",
    "    Good balance of speed and quality, great for general image understanding\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "        self.display_name = \"Qwen2-VL-2B\"\n",
    "        self.vram_estimate = \"5-6 GB\"\n",
    "        \n",
    "        print(f\"Loading {self.display_name}...\")\n",
    "        self.processor = AutoProcessor.from_pretrained(self.model_name, trust_remote_code=True)\n",
    "        self.model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "            self.model_name,\n",
    "            dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        print(f\"Loaded {self.display_name}\")\n",
    "    \n",
    "    def generate_caption(self, image):\n",
    "        try:\n",
    "            messages = [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": image},\n",
    "                    {\"type\": \"text\", \"text\": \"Describe this image in detail.\"}\n",
    "                ]\n",
    "            }]\n",
    "            \n",
    "            text = self.processor.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            \n",
    "            image_inputs, video_inputs = process_vision_info(messages)\n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            \n",
    "            inputs = self.processor(\n",
    "                text=[text], images=image_inputs, videos=video_inputs,\n",
    "                padding=True, return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(**inputs, max_new_tokens=256)\n",
    "            \n",
    "            full_output = self.processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "            \n",
    "            if \"assistant\" in full_output:\n",
    "                caption = full_output.split(\"assistant\")[-1].strip()\n",
    "            else:\n",
    "                caption = full_output\n",
    "            \n",
    "            return caption\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)[:100]}\"\n",
    "    \n",
    "    def unload(self):\n",
    "        del self.model\n",
    "        del self.processor\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "print(f\"Qwen2-VL-2B class ready (VRAM: 5-6 GB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VLM Model 2: MobileVLM V2 (3B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileVLMV2:\n",
    "    \"\"\"MobileVLM V2 (3B) - Optimized for mobile/edge devices\n",
    "    Lightweight but still provides decent quality, great for low-latency applications\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model_name = \"mtgv/MobileVLM_V2-3B\"\n",
    "        self.display_name = \"MobileVLM-V2-3B\"\n",
    "        self.vram_estimate = \"6-8 GB\"\n",
    "        \n",
    "        print(f\"Loading {self.display_name}...\")\n",
    "        from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        print(f\"Loaded {self.display_name}\")\n",
    "    \n",
    "    def generate_caption(self, image):\n",
    "        try:\n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            \n",
    "            # MobileVLM uses different format\n",
    "            prompt = \"Describe this image in detail.\"\n",
    "            \n",
    "            inputs = self.tokenizer(\n",
    "                prompt, images=image, return_tensors='pt'\n",
    "            ).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(**inputs, max_new_tokens=256)\n",
    "            \n",
    "            caption = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            return caption\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)[:100]}\"\n",
    "    \n",
    "    def unload(self):\n",
    "        del self.model\n",
    "        del self.tokenizer\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "print(f\"MobileVLM V2 class ready (VRAM: 6-8 GB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VLM Model 3: LLaVA-1.5 (7B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLaVA15:\n",
    "    \"\"\"LLaVA-1.5 (7B) - Popular open-source model by Meta researchers\n",
    "    Larger model with better reasoning and understanding capabilities\n",
    "    Tight fit on 16GB VRAM but works with proper memory management\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model_name = \"llava-hf/llava-1.5-7b-hf\"\n",
    "        self.display_name = \"LLaVA-1.5-7B\"\n",
    "        self.vram_estimate = \"14-16 GB\"\n",
    "        \n",
    "        print(f\"Loading {self.display_name}...\")\n",
    "        from transformers import LlavaProcessor, LlavaForConditionalGeneration\n",
    "        \n",
    "        self.processor = LlavaProcessor.from_pretrained(self.model_name)\n",
    "        self.model = LlavaForConditionalGeneration.from_pretrained(\n",
    "            self.model_name,\n",
    "            dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        print(f\"Loaded {self.display_name}\")\n",
    "    \n",
    "    def generate_caption(self, image):\n",
    "        try:\n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            \n",
    "            prompt = \"USER: <image>\\nDescribe this image in detail.\\nASSISTANT:\"\n",
    "            inputs = self.processor(prompt, image, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(**inputs, max_new_tokens=256)\n",
    "            \n",
    "            caption = self.processor.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            if \"ASSISTANT:\" in caption:\n",
    "                caption = caption.split(\"ASSISTANT:\")[-1].strip()\n",
    "            \n",
    "            return caption\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)[:100]}\"\n",
    "    \n",
    "    def unload(self):\n",
    "        del self.model\n",
    "        del self.processor\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "print(f\"LLaVA-1.5 class ready (VRAM: 14-16 GB - largest model, tight fit)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VLM Model 4: Phi-3-Vision (4.2B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phi3Vision:\n",
    "    \"\"\"Phi-3-Vision (4.2B) - Microsoft's efficient model with good reasoning\n",
    "    Sweet spot between quality and efficiency, excellent for reasoning tasks\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model_name = \"microsoft/Phi-3-vision-128k-instruct\"\n",
    "        self.display_name = \"Phi-3-Vision-4.2B\"\n",
    "        self.vram_estimate = \"8-10 GB\"\n",
    "        \n",
    "        print(f\"Loading {self.display_name}...\")\n",
    "        from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "        \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model_name, trust_remote_code=True\n",
    "        )\n",
    "        print(f\"Loaded {self.display_name}\")\n",
    "    \n",
    "    def generate_caption(self, image):\n",
    "        try:\n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            \n",
    "            messages = [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"<|image_1|>\\nDescribe this image in detail.\",\n",
    "            }]\n",
    "            \n",
    "            text = self.tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            \n",
    "            inputs = self.tokenizer(\n",
    "                text, return_tensors=\"pt\", padding=True\n",
    "            ).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(**inputs, max_new_tokens=256)\n",
    "            \n",
    "            caption = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            return caption\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)[:100]}\"\n",
    "    \n",
    "    def unload(self):\n",
    "        del self.model\n",
    "        del self.tokenizer\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "print(f\"Phi-3-Vision class ready (VRAM: 8-10 GB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VLM Model 5: InternVL2 (2B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InternVL2:\n",
    "    \"\"\"InternVL2 (2B) - OpenGVLab's compact and efficient model\n",
    "    Very lightweight but surprisingly strong performance for its size\n",
    "    Great for resource-constrained scenarios\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model_name = \"OpenGVLab/InternVL2-2B\"\n",
    "        self.display_name = \"InternVL2-2B\"\n",
    "        self.vram_estimate = \"4-6 GB\"\n",
    "        \n",
    "        print(f\"Loading {self.display_name}...\")\n",
    "        from transformers import AutoModel, AutoTokenizer\n",
    "        \n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            self.model_name,\n",
    "            dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model_name, trust_remote_code=True\n",
    "        )\n",
    "        print(f\"Loaded {self.display_name}\")\n",
    "    \n",
    "    def generate_caption(self, image):\n",
    "        try:\n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            \n",
    "            prompt = \"Describe this image in detail.\"\n",
    "            text = f\"<image>\\n{prompt}\"\n",
    "            \n",
    "            inputs = self.tokenizer(\n",
    "                text, images=image, return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(**inputs, max_new_tokens=256)\n",
    "            \n",
    "            caption = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            return caption\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)[:100]}\"\n",
    "    \n",
    "    def unload(self):\n",
    "        del self.model\n",
    "        del self.tokenizer\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "print(f\"InternVL2 class ready (VRAM: 4-6 GB - one of the smallest)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VLM Model 6: SmolVLM2 (2.2B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmolVLM2:\n",
    "    \"\"\"SmolVLM2 (2.2B) - HuggingFace's ultra-efficient model\n",
    "    Unique feature: supports both images AND videos in same model\n",
    "    Excellent efficiency, leaves plenty of VRAM headroom\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model_name = \"HuggingFaceTB/SmolVLM2-2.2B-Instruct\"\n",
    "        self.display_name = \"SmolVLM2-2.2B\"\n",
    "        self.vram_estimate = \"5-5.2 GB\"\n",
    "        \n",
    "        print(f\"Loading {self.display_name}...\")\n",
    "        from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "        \n",
    "        self.processor = AutoProcessor.from_pretrained(\n",
    "            self.model_name, trust_remote_code=True\n",
    "        )\n",
    "        self.model = AutoModelForVision2Seq.from_pretrained(\n",
    "            self.model_name,\n",
    "            dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        print(f\"Loaded {self.display_name}\")\n",
    "    \n",
    "    def generate_caption(self, image):\n",
    "        try:\n",
    "            messages = [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": \"Describe this image in detail.\"}\n",
    "                ]\n",
    "            }]\n",
    "            \n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            \n",
    "            text = self.processor.apply_chat_template(\n",
    "                messages, add_generation_prompt=True\n",
    "            )\n",
    "            \n",
    "            inputs = self.processor(\n",
    "                text=text, images=[image], return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(**inputs, max_new_tokens=256)\n",
    "            \n",
    "            caption = self.processor.decode(outputs[0], skip_special_tokens=True)\n",
    "            return caption\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)[:100]}\"\n",
    "    \n",
    "    def unload(self):\n",
    "        del self.model\n",
    "        del self.processor\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "print(f\"SmolVLM2 class ready (VRAM: 5.2 GB - efficient with video support)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VLM Model 7: DeepSeek-VL (1.3B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSeekVL:\n",
    "    \"\"\"DeepSeek-VL (1.3B) - Original DeepSeek vision model\n",
    "    Very lightweight, good for text in images and OCR tasks\n",
    "    Great for document understanding and text extraction\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model_name = \"deepseek-ai/deepseek-vl-1.3b-chat\"\n",
    "        self.display_name = \"DeepSeek-VL-1.3B\"\n",
    "        self.vram_estimate = \"4-5 GB\"\n",
    "        \n",
    "        print(f\"Loading {self.display_name}...\")\n",
    "        from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "        \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model_name, trust_remote_code=True\n",
    "        )\n",
    "        print(f\"Loaded {self.display_name}\")\n",
    "    \n",
    "    def generate_caption(self, image):\n",
    "        try:\n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            \n",
    "            # DeepSeek uses specific image token format\n",
    "            image_tokens = \"<|vision_start|><|image_pad|><|vision_end|>\"\n",
    "            prompt = f\"{image_tokens}\\nDescribe this image in detail.\"\n",
    "            \n",
    "            # DeepSeek tokenizer expects special handling for images\n",
    "            inputs = self.tokenizer.encode_plus(\n",
    "                prompt, images=[image], return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(**inputs, max_new_tokens=256)\n",
    "            \n",
    "            caption = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            return caption\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)[:100]}\"\n",
    "    \n",
    "    def unload(self):\n",
    "        del self.model\n",
    "        del self.tokenizer\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "print(f\"DeepSeek-VL class ready (VRAM: 4-5 GB - smallest, good for OCR)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration: Select Which Models to Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all available models\n",
    "AVAILABLE_MODELS = {\n",
    "    \"Qwen2-VL-2B\": QwenVLM,\n",
    "    \"MobileVLM-V2-3B\": MobileVLMV2,\n",
    "    \"LLaVA-1.5-7B\": LLaVA15,\n",
    "    \"Phi-3-Vision-4.2B\": Phi3Vision,\n",
    "    \"InternVL2-2B\": InternVL2,\n",
    "    \"SmolVLM2-2.2B\": SmolVLM2,\n",
    "    \"DeepSeek-VL-1.3B\": DeepSeekVL,\n",
    "}\n",
    "\n",
    "# ========================================\n",
    "# CUSTOMIZE THIS LIST TO SELECT MODELS\n",
    "# ========================================\n",
    "# Comment/uncomment models to enable/disable\n",
    "MODELS_TO_RUN = [\n",
    "    \"Qwen2-VL-2B\",          # Fast, good quality, 5-6 GB\n",
    "    # \"MobileVLM-V2-3B\",     # Lightweight, 6-8 GB\n",
    "    # \"LLaVA-1.5-7B\",        # Larger, better reasoning, 14-16 GB (tight!)\n",
    "    # \"Phi-3-Vision-4.2B\",   # Sweet spot, 8-10 GB\n",
    "    # \"InternVL2-2B\",        # Very compact, 4-6 GB\n",
    "    # \"SmolVLM2-2.2B\",       # Efficient with video support, 5.2 GB\n",
    "    # \"DeepSeek-VL-1.3B\",    # Smallest, good OCR, 4-5 GB\n",
    "]\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"MODELS CONFIGURATION\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nModels to run: {len(MODELS_TO_RUN)}\")\n",
    "for model_name in MODELS_TO_RUN:\n",
    "    print(f\"  ✓ {model_name}\")\n",
    "\n",
    "print(f\"\\nAvailable models not selected: {len(AVAILABLE_MODELS) - len(MODELS_TO_RUN)}\")\n",
    "for model_name in AVAILABLE_MODELS:\n",
    "    if model_name not in MODELS_TO_RUN:\n",
    "        print(f\"  ○ {model_name}\")\n",
    "print(f\"\\nEdit MODELS_TO_RUN list above to change selection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process All Selected Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "\n",
    "for model_name in MODELS_TO_RUN:\n",
    "    if model_name not in AVAILABLE_MODELS:\n",
    "        print(f\"Warning: {model_name} not found in available models\")\n",
    "        continue\n",
    "    \n",
    "    model_class = AVAILABLE_MODELS[model_name]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Processing with: {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        # Load model\n",
    "        model = model_class()\n",
    "        \n",
    "        model_results = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Process images\n",
    "        for idx, img_file in enumerate(tqdm(image_files, desc=f\"Processing with {model_name}\")):\n",
    "            try:\n",
    "                img_path = os.path.join(IMAGES_DIR, img_file)\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "                \n",
    "                img_id = img_file.replace('.jpg', '')\n",
    "                img_size = image.size\n",
    "                \n",
    "                start_inference = time.time()\n",
    "                caption = model.generate_caption(image)\n",
    "                inference_time = time.time() - start_inference\n",
    "                \n",
    "                result = {\n",
    "                    'image_id': img_id,\n",
    "                    'model_name': model_name,\n",
    "                    'caption': caption,\n",
    "                    'processing_time_sec': round(inference_time, 2),\n",
    "                    'image_width': img_size[0],\n",
    "                    'image_height': img_size[1],\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "                \n",
    "                model_results.append(result)\n",
    "                all_results.append(result)\n",
    "                \n",
    "                # Checkpoint every 50 images\n",
    "                if (idx + 1) % 50 == 0:\n",
    "                    checkpoint_df = pd.DataFrame(model_results)\n",
    "                    checkpoint_path = os.path.join(\n",
    "                        RESULTS_DIR,\n",
    "                        f\"checkpoint_{model_name.replace('/', '_')}_{idx+1}.csv\"\n",
    "                    )\n",
    "                    checkpoint_df.to_csv(checkpoint_path, index=False)\n",
    "                    elapsed = (time.time() - start_time) / 60\n",
    "                    tqdm.write(f\"Checkpoint {idx+1}/{len(image_files)}: {elapsed:.1f}m elapsed\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                tqdm.write(f\"Error processing {img_file}: {str(e)[:50]}\")\n",
    "                continue\n",
    "        \n",
    "        # Save model results\n",
    "        model_df = pd.DataFrame(model_results)\n",
    "        result_path = os.path.join(RESULTS_DIR, f\"results_{model_name.replace('/', '_')}.csv\")\n",
    "        model_df.to_csv(result_path, index=False)\n",
    "        \n",
    "        elapsed_time = (time.time() - start_time) / 60\n",
    "        successful = len([r for r in model_results if not r['caption'].startswith('Error')])\n",
    "        \n",
    "        print(f\"\\nCompleted {model_name}:\")\n",
    "        print(f\"  Processed: {len(model_results)}/{len(image_files)} images\")\n",
    "        print(f\"  Successful: {successful}/{len(model_results)}\")\n",
    "        print(f\"  Time: {elapsed_time:.1f} minutes\")\n",
    "        if len(model_results) > 0:\n",
    "            print(f\"  Avg per image: {elapsed_time * 60 / len(model_results):.1f}s\")\n",
    "        print(f\"  Saved to: {result_path}\")\n",
    "        \n",
    "        # Unload model to free VRAM\n",
    "        model.unload()\n",
    "        time.sleep(2)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {model_name}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Save combined results\n",
    "if all_results:\n",
    "    combined_df = pd.DataFrame(all_results)\n",
    "    combined_path = os.path.join(RESULTS_DIR, 'all_models_comparison.csv')\n",
    "    combined_df.to_csv(combined_path, index=False)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ALL PROCESSING COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total results: {len(all_results)}\")\n",
    "    print(f\"Saved to: {combined_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "combined_path = os.path.join(RESULTS_DIR, 'all_models_comparison.csv')\nif os.path.exists(combined_path):\n    results_df = pd.read_csv(combined_path)\n    \n    print(f\"\\n{'='*80}\")\n    print(\"DETAILED RESULTS SUMMARY\")\n    print(f\"{'='*80}\")\n    \n    for model_name in results_df['model_name'].unique():\n        model_data = results_df[results_df['model_name'] == model_name]\n        caption_lengths = model_data['caption'].str.len()\n        \n        successful = len([c for c in model_data['caption'] if not c.startswith('Error')])\n        \n        print(f\"\\n{model_name}:\")\n        print(f\"  Total images: {len(model_data)}\")\n        print(f\"  Successful: {successful}/{len(model_data)}\")\n        print(f\"  Success rate: {successful/len(model_data)*100:.1f}%\")\n        print(f\"  Avg caption length: {caption_lengths.mean():.0f} characters\")\n        print(f\"  Caption range: {caption_lengths.min()}-{caption_lengths.max()}\")\n        print(f\"  Avg inference time: {model_data['processing_time_sec'].mean():.2f}s\")\n        print(f\"  Min/Max inference: {model_data['processing_time_sec'].min():.2f}s / {model_data['processing_time_sec'].max():.2f}s\")\n    \n    # Export to both CSV and JSON\n    json_path = os.path.join(RESULTS_DIR, 'all_models_comparison.json')\n    results_df.to_json(json_path, orient='records', indent=2)\n    \n    print(f\"\\n{'='*80}\")\n    print(\"RESULTS EXPORTED\")\n    print(f\"{'='*80}\")\n    print(f\"CSV: {combined_path}\")\n    print(f\"JSON: {json_path}\")\n    print(f\"{'='*80}\\n\")\nelse:\n    print(\"No results file found. Run processing section first.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_path = os.path.join(RESULTS_DIR, 'all_models_comparison.csv')\n",
    "if os.path.exists(combined_path):\n",
    "    results_df = pd.read_csv(combined_path)\n",
    "    \n",
    "    # Show results for first 2 unique images\n",
    "    unique_images = results_df['image_id'].unique()[:2]\n",
    "    \n",
    "    for img_id in unique_images:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Image ID: {img_id}\")\n",
    "        img_data = results_df[results_df['image_id'] == img_id]\n",
    "        \n",
    "        if len(img_data) > 0:\n",
    "            first_row = img_data.iloc[0]\n",
    "            print(f\"Size: {first_row['image_width']}x{first_row['image_height']}\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            for _, row in img_data.iterrows():\n",
    "                print(f\"\\n{row['model_name']} ({row['processing_time_sec']:.2f}s):\")\n",
    "                caption = row['caption']\n",
    "                if len(caption) > 400:\n",
    "                    caption = caption[:400] + \"...\"\n",
    "                print(f\"{caption}\")\n",
    "else:\n",
    "    print(\"No results available. Run processing section first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD CODE - Gemini Processing (Commented Out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Using Gemini API for image captioning\n",
    "# ============================================\n",
    "# Switched to using open-source models instead for better control\n",
    "# Keeping this code for reference - shows what we replaced\n",
    "\n",
    "# import google.generativeai as genai\n",
    "#\n",
    "# GEMINI_API_KEY = \"your_api_key_here\"\n",
    "# genai.configure(api_key=GEMINI_API_KEY)\n",
    "#\n",
    "# def generate_gemini_caption(image):\n",
    "#     \"\"\"Generate caption using Gemini 2.5 Pro\"\"\"\n",
    "#     try:\n",
    "#         gemini_model = genai.GenerativeModel('models/gemini-2.5-pro')\n",
    "#         response = gemini_model.generate_content([\n",
    "#             \"Describe this image in detail.\",\n",
    "#             image\n",
    "#         ])\n",
    "#         return response.text\n",
    "#     except Exception as e:\n",
    "#         return f\"Error: {str(e)}\"\n",
    "# ============================================\n",
    "# Old manual patch extraction approach\n",
    "# ============================================\n",
    "# Modern VLMs handle preprocessing internally, no need for manual patches\n",
    "#\n",
    "# def extract_patches(image, patch_size=224, stride=112):\n",
    "#     patches = []\n",
    "#     width, height = image.size\n",
    "#     for y in range(0, height - patch_size + 1, stride):\n",
    "#         for x in range(0, width - patch_size + 1, stride):\n",
    "#             patch = image.crop((x, y, x + patch_size, y + patch_size))\n",
    "#             patches.append(patch)\n",
    "#     return patches\n",
    "\n",
    "print(\"Old code sections kept for reference but not used\")\n",
    "print(\"We're now using 7 different open-source VLM models instead\")\n",
    "print(\"Benefits: No API costs, no rate limiting, full local control, easy to compare\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python3",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}