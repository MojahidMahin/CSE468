{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-cell",
   "metadata": {},
   "source": [
    "# Multi-VLM Image Captioning Framework\n",
    "## Comparing 7 Vision-Language Models on COCO Dataset\n",
    "This notebook compares multiple lightweight VLM models optimized for RTX 5080 (16GB VRAM)"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-setup-imports",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T23:25:16.573017Z",
     "start_time": "2025-11-17T23:23:36.655689Z"
    }
   },
   "source": [
    "# Only needed if running on Google Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# Install required packages for vision-language models\n",
    "!pip install transformers torch torchvision Pillow datasets pycocotools\n",
    "!pip install -q accelerate bitsandbytes qwen-vl-utils"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./.venv/lib/python3.10/site-packages (4.57.1)\r\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.10/site-packages (2.9.1)\r\n",
      "Collecting torchvision\r\n",
      "  Using cached torchvision-0.24.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\r\n",
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.10/site-packages (12.0.0)\r\n",
      "Collecting datasets\r\n",
      "  Using cached datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\r\n",
      "Requirement already satisfied: pycocotools in ./.venv/lib/python3.10/site-packages (2.0.10)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from transformers) (3.20.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.venv/lib/python3.10/site-packages (from transformers) (0.36.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.10/site-packages (from transformers) (2.2.6)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from transformers) (25.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from transformers) (6.0.3)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from transformers) (2025.11.3)\r\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from transformers) (2.32.5)\r\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.venv/lib/python3.10/site-packages (from transformers) (0.22.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.10/site-packages (from transformers) (0.6.2)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.10/site-packages (from transformers) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.10/site-packages (from torch) (4.15.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.10/site-packages (from torch) (1.14.0)\r\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.10/site-packages (from torch) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./.venv/lib/python3.10/site-packages (from torch) (2025.10.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.10/site-packages (from torch) (12.8.93)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.10/site-packages (from torch) (12.8.90)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.10/site-packages (from torch) (12.8.90)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.10/site-packages (from torch) (9.10.2.21)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.10/site-packages (from torch) (12.8.4.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.10/site-packages (from torch) (11.3.3.83)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.10/site-packages (from torch) (10.3.9.90)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.10/site-packages (from torch) (11.7.3.90)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.10/site-packages (from torch) (12.5.8.93)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.10/site-packages (from torch) (0.7.1)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./.venv/lib/python3.10/site-packages (from torch) (2.27.5)\r\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./.venv/lib/python3.10/site-packages (from torch) (3.3.20)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.10/site-packages (from torch) (12.8.90)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.10/site-packages (from torch) (12.8.93)\r\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.10/site-packages (from torch) (1.13.1.3)\r\n",
      "Requirement already satisfied: triton==3.5.1 in ./.venv/lib/python3.10/site-packages (from torch) (3.5.1)\r\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\r\n",
      "  Using cached pyarrow-22.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\r\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\r\n",
      "  Using cached dill-0.4.0-py3-none-any.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (from datasets) (2.3.3)\r\n",
      "Requirement already satisfied: httpx<1.0.0 in ./.venv/lib/python3.10/site-packages (from datasets) (0.28.1)\r\n",
      "Collecting xxhash (from datasets)\r\n",
      "  Using cached xxhash-3.6.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\r\n",
      "Collecting multiprocess<0.70.19 (from datasets)\r\n",
      "  Using cached multiprocess-0.70.18-py310-none-any.whl.metadata (7.5 kB)\r\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\r\n",
      "  Using cached aiohttp-3.13.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\r\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (4.11.0)\r\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\r\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (1.0.9)\r\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (3.11)\r\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (3.4.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (2.5.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch) (3.0.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\r\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\r\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\r\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\r\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\r\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\r\n",
      "  Using cached async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\r\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\r\n",
      "  Using cached frozenlist-1.8.0-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\r\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\r\n",
      "  Using cached multidict-6.7.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\r\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\r\n",
      "  Using cached propcache-0.4.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\r\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\r\n",
      "  Using cached yarl-1.22.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\r\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\r\n",
      "Using cached torchvision-0.24.1-cp310-cp310-manylinux_2_28_x86_64.whl (8.0 MB)\r\n",
      "Using cached datasets-4.4.1-py3-none-any.whl (511 kB)\r\n",
      "Using cached dill-0.4.0-py3-none-any.whl (119 kB)\r\n",
      "Using cached multiprocess-0.70.18-py310-none-any.whl (134 kB)\r\n",
      "Downloading pyarrow-22.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (47.6 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m47.6/47.6 MB\u001B[0m \u001B[31m1.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m0m\r\n",
      "\u001B[?25hUsing cached xxhash-3.6.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\r\n",
      "Downloading aiohttp-3.13.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m1.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hUsing cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\r\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\r\n",
      "Using cached async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\r\n",
      "Downloading frozenlist-1.8.0-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (219 kB)\r\n",
      "Downloading multidict-6.7.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (241 kB)\r\n",
      "Downloading propcache-0.4.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (196 kB)\r\n",
      "Downloading yarl-1.22.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (346 kB)\r\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multidict, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, torchvision, datasets\r\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 async-timeout-5.0.1 datasets-4.4.1 dill-0.4.0 frozenlist-1.8.0 multidict-6.7.0 multiprocess-0.70.18 propcache-0.4.1 pyarrow-22.0.0 torchvision-0.24.1 xxhash-3.6.0 yarl-1.22.0\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "section-imports",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-imports",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T23:25:18.152529Z",
     "start_time": "2025-11-17T23:25:16.593329Z"
    }
   },
   "source": "from pycocotools.coco import COCO\nimport requests\nimport os\nfrom PIL import Image\nimport json\nimport pandas as pd\nfrom tqdm import tqdm\nimport time\nimport torch\nimport gc\nfrom datetime import datetime\n\n# Configuration\nANNOTATIONS_PATH = '/home/vortex/CSE 468 AFE/Project/annotations'\nIMAGES_DIR = 'coco_images'\nRESULTS_DIR = 'results'\nNUM_IMAGES = 1000  # Processing 1000 images\n\n# Create necessary directories\nos.makedirs(IMAGES_DIR, exist_ok=True)\nos.makedirs(RESULTS_DIR, exist_ok=True)\n\nprint(f\"Setup complete. Using annotations from: {ANNOTATIONS_PATH}\")\nprint(f\"GPU Available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "section-dataset",
   "metadata": {},
   "source": [
    "## Load COCO Dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-load-coco",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T00:08:12.130906Z",
     "start_time": "2025-11-17T23:25:18.253215Z"
    }
   },
   "source": [
    "# Load COCO annotations from existing local copy\n",
    "coco = COCO(os.path.join(ANNOTATIONS_PATH, 'captions_val2017.json'))\n",
    "\n",
    "# Get 200 random image IDs for evaluation\n",
    "import random\n",
    "random.seed(42)\n",
    "all_img_ids = coco.getImgIds()\n",
    "selected_img_ids = random.sample(all_img_ids, NUM_IMAGES)\n",
    "\n",
    "print(f\"Selected {len(selected_img_ids)} images for processing\")\n",
    "\n",
    "# Download images if they don't exist\n",
    "missing_count = 0\n",
    "for img_id in tqdm(selected_img_ids, desc=\"Checking images\"):\n",
    "    img_info = coco.loadImgs(img_id)[0]\n",
    "    img_path = os.path.join(IMAGES_DIR, img_info['file_name'])\n",
    "    \n",
    "    if not os.path.exists(img_path):\n",
    "        try:\n",
    "            img_url = img_info['coco_url']\n",
    "            img_data = requests.get(img_url).content\n",
    "            with open(img_path, 'wb') as f:\n",
    "                f.write(img_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {img_info['file_name']}: {e}\")\n",
    "            missing_count += 1\n",
    "\n",
    "print(f\"Downloaded {NUM_IMAGES - missing_count}/{NUM_IMAGES} images\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.09s)\n",
      "creating index...\n",
      "index created!\n",
      "Selected 1000 images for processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking images: 100%|██████████| 1000/1000 [42:53<00:00,  2.57s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 1000/1000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "section-patch-extract",
   "metadata": {},
   "source": [
    "## Image Patch Extraction (Optional - Not used with modern VLMs)"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-patch-extraction",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T00:08:12.223643Z",
     "start_time": "2025-11-18T00:08:12.221297Z"
    }
   },
   "source": [
    "# Modern VLMs handle image preprocessing internally, so we don't need manual patch extraction\n",
    "# Keeping this function for reference in case needed for future experiments\n",
    "\n",
    "# def extract_patches(image, patch_size=224, stride=112):\n",
    "#     Extract overlapping patches from image\n",
    "#     patches = []\n",
    "#     width, height = image.size\n",
    "#     for y in range(0, height - patch_size + 1, stride):\n",
    "#         for x in range(0, width - patch_size + 1, stride):\n",
    "#             patch = image.crop((x, y, x + patch_size, y + patch_size))\n",
    "#             patches.append(patch)\n",
    "#     return patches\n",
    "\n",
    "# Test on first image\n",
    "# test_img = Image.open(os.path.join(IMAGES_DIR, os.listdir(IMAGES_DIR)[0]))\n",
    "# patches = extract_patches(test_img)\n",
    "# print(f\"Extracted {len(patches)} patches from test image\")\n",
    "\n",
    "print(\"Patch extraction code is available but commented out since models handle resizing internally\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch extraction code is available but commented out since models handle resizing internally\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "section-qwen",
   "metadata": {},
   "source": [
    "## VLM Model 1: Qwen2-VL-2B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-qwen-setup",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T00:18:59.677598Z",
     "start_time": "2025-11-18T00:08:12.268651Z"
    }
   },
   "source": "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\n\nclass QwenVLM:\n    \"\"\"Qwen2-VL-2B - Alibaba's compact vision-language model\n    Optimized for 1000 image processing with reduced tokens for speed\"\"\"\n    \n    def __init__(self):\n        self.model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n        self.display_name = \"Qwen2-VL-2B\"\n        self.vram_estimate = \"5-6 GB\"\n        \n        print(f\"Loading {self.display_name}...\")\n        self.processor = AutoProcessor.from_pretrained(self.model_name, trust_remote_code=True)\n        self.model = Qwen2VLForConditionalGeneration.from_pretrained(\n            self.model_name,\n            dtype=torch.float16,\n            device_map=\"auto\",\n            trust_remote_code=True\n        )\n        print(f\"Loaded {self.display_name}\")\n    \n    def generate_caption(self, image, max_tokens=128):\n        \"\"\"Generate caption for single image\n        Reduced from 256 to 128 tokens for faster inference on 1000 images\"\"\"\n        try:\n            messages = [{\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"image\", \"image\": image},\n                    {\"type\": \"text\", \"text\": \"Describe this image briefly.\"}  # Changed to 'briefly' for shorter captions\n                ]\n            }]\n            \n            text = self.processor.apply_chat_template(\n                messages,\n                tokenize=False,\n                add_generation_prompt=True\n            )\n            \n            image_inputs, video_inputs = process_vision_info(messages)\n            \n            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n            inputs = self.processor(\n                text=[text],\n                images=image_inputs,\n                videos=video_inputs,\n                padding=True,\n                return_tensors=\"pt\"\n            ).to(device)\n            \n            with torch.no_grad():\n                outputs = self.model.generate(**inputs, max_new_tokens=max_tokens)\n            \n            full_output = self.processor.batch_decode(outputs, skip_special_tokens=True)[0]\n            \n            if \"assistant\" in full_output:\n                caption = full_output.split(\"assistant\")[-1].strip()\n            else:\n                caption = full_output\n            \n            return caption\n        except Exception as e:\n            return f\"Error: {str(e)[:100]}\"\n    \n    def unload(self):\n        \"\"\"Free up GPU memory\"\"\"\n        del self.model\n        del self.processor\n        torch.cuda.empty_cache()\n        gc.collect()\n\n# Load model\nprint(\"Initializing Qwen model - this may take 2-3 minutes on first run...\")\nqwen_model = QwenVLM()\nprint(f\"Estimated VRAM usage: {qwen_model.vram_estimate}\")\nprint(f\"Ready to process 1000 images (~2-3 seconds per image = 33-50 minutes total)\")",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|██████████| 2/2 [10:27<00:00, 313.58s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Qwen2-VL-2B\n",
      "Estimated VRAM usage: 5-6 GB\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "section-processing",
   "metadata": {},
   "source": [
    "## Multi-Model Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-process-models",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T00:18:59.739837Z",
     "start_time": "2025-11-18T00:18:59.736205Z"
    }
   },
   "source": [
    "# Define all models with their classes and parameters\n",
    "MODELS_CONFIG = [\n",
    "    {\"name\": \"Qwen2-VL-2B\", \"enabled\": True},\n",
    "]\n",
    "\n",
    "# Get list of images to process\n",
    "image_files = [f for f in os.listdir(IMAGES_DIR) if f.endswith('.jpg')]\n",
    "image_files = image_files[:NUM_IMAGES]\n",
    "\n",
    "print(f\"Ready to process {len(image_files)} images\")\n",
    "print(f\"Expected time per image: 2-3 seconds with Qwen\")\n",
    "print(f\"Total estimated time: {len(image_files) * 2.5 / 60:.1f} minutes\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to process 1000 images\n",
      "Expected time per image: 2-3 seconds with Qwen\n",
      "Total estimated time: 41.7 minutes\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "cell-run-processing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T01:45:55.182399Z",
     "start_time": "2025-11-18T00:18:59.783975Z"
    }
   },
   "source": "import warnings\nwarnings.filterwarnings('ignore')\n\n# Process with Qwen model - optimized for 1000 images\nall_results = []\n\nmodel_name = \"Qwen2-VL-2B\"\nprint(f\"\\n{'='*80}\")\nprint(f\"Processing 1000 images with: {model_name}\")\nprint(f\"{'='*80}\")\nprint(f\"Estimated time: 33-50 minutes\")\nprint(f\"Checkpoints will be saved every 100 images\")\nprint(f\"{'='*80}\\n\")\n\nmodel_results = []\nstart_time = time.time()\n\nfor idx, img_file in enumerate(tqdm(image_files, desc=f\"Processing with {model_name}\")):\n    try:\n        img_path = os.path.join(IMAGES_DIR, img_file)\n        image = Image.open(img_path).convert('RGB')\n        \n        img_id = img_file.replace('.jpg', '')\n        img_size = image.size\n        \n        start_inference = time.time()\n        caption = qwen_model.generate_caption(image, max_tokens=128)\n        inference_time = time.time() - start_inference\n        \n        result = {\n            'image_id': img_id,\n            'model_name': model_name,\n            'caption': caption,\n            'processing_time_sec': round(inference_time, 2),\n            'image_width': img_size[0],\n            'image_height': img_size[1],\n            'timestamp': datetime.now().isoformat()\n        }\n        \n        model_results.append(result)\n        all_results.append(result)\n        \n        # Save checkpoint every 100 images (more frequent for 1000 image run)\n        if (idx + 1) % 100 == 0:\n            checkpoint_df = pd.DataFrame(model_results)\n            checkpoint_path = os.path.join(RESULTS_DIR, f\"checkpoint_{model_name}_{idx+1}.csv\")\n            checkpoint_df.to_csv(checkpoint_path, index=False)\n            elapsed = (time.time() - start_time) / 60\\n            avg_time = elapsed / (idx + 1)\\n            remaining = avg_time * (len(image_files) - idx - 1) / 60\\n            tqdm.write(f\\\"✓ Checkpoint {idx+1}/1000: {elapsed:.1f}m elapsed, ~{remaining:.0f}m remaining\\\")\\n    \\n    except Exception as e:\\n        tqdm.write(f\\\"Error processing {img_file}: {str(e)[:50]}\\\")\\n        continue\\n\\nmodel_df = pd.DataFrame(model_results)\\nresult_path = os.path.join(RESULTS_DIR, f\\\"results_{model_name}.csv\\\")\\nmodel_df.to_csv(result_path, index=False)\\n\\nelapsed_time = (time.time() - start_time) / 60\\nsuccessful = len([r for r in model_results if not r['caption'].startswith('Error')])\\n\\nprint(f\\\"\\\\n{'='*80}\\\")\\nprint(f\\\"COMPLETED {model_name}\\\")\\nprint(f\\\"{'='*80}\\\")\\nprint(f\\\"Processed: {len(model_results)}/1000 images\\\")\\nprint(f\\\"Successful: {successful}/{len(model_results)} ({successful/len(model_results)*100:.1f}%)\\\")\\nprint(f\\\"Total time: {elapsed_time:.1f} minutes ({elapsed_time/60:.1f} hours)\\\")\\nprint(f\\\"Avg per image: {elapsed_time * 60 / len(model_results):.2f}s\\\")\\nprint(f\\\"Results saved to: {result_path}\\\")\\nprint(f\\\"{'='*80}\\\")\\n\\nqwen_model.unload()\\n\\nif all_results:\\n    combined_df = pd.DataFrame(all_results)\\n    combined_path = os.path.join(RESULTS_DIR, 'all_models_comparison.csv')\\n    combined_df.to_csv(combined_path, index=False)\\n    \\n    print(f\\\"\\\\nCombined results saved to: {combined_path}\\\")\\n    print(f\\\"Total results in database: {len(all_results)}\\\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Processing with: Qwen2-VL-2B\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing with Qwen2-VL-2B:  15%|█▌        | 150/1000 [13:48<1:14:31,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 150/1000: 13.8m elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing with Qwen2-VL-2B:  20%|██        | 200/1000 [18:07<1:06:59,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 200/1000: 18.1m elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing with Qwen2-VL-2B:  25%|██▌       | 250/1000 [22:36<1:13:34,  5.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 250/1000: 22.6m elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing with Qwen2-VL-2B:  30%|███       | 300/1000 [27:02<1:02:42,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 300/1000: 27.0m elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing with Qwen2-VL-2B:  35%|███▌      | 350/1000 [31:21<1:00:22,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 350/1000: 31.4m elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing with Qwen2-VL-2B:  40%|████      | 400/1000 [35:29<55:09,  5.52s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 400/1000: 35.5m elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing with Qwen2-VL-2B:  45%|████▌     | 450/1000 [39:53<44:00,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 450/1000: 39.9m elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing with Qwen2-VL-2B:  50%|█████     | 500/1000 [44:14<43:38,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 500/1000: 44.2m elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing with Qwen2-VL-2B:  55%|█████▌    | 550/1000 [48:18<31:25,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 550/1000: 48.3m elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing with Qwen2-VL-2B:  60%|██████    | 600/1000 [52:44<37:20,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 600/1000: 52.7m elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing with Qwen2-VL-2B:  65%|██████▌   | 650/1000 [57:06<26:11,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 650/1000: 57.1m elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing with Qwen2-VL-2B:  70%|███████   | 700/1000 [1:01:30<27:12,  5.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 700/1000: 61.5m elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing with Qwen2-VL-2B:  75%|███████▌  | 750/1000 [1:05:43<20:00,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 750/1000: 65.7m elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing with Qwen2-VL-2B:  80%|████████  | 800/1000 [1:09:49<14:54,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 800/1000: 69.8m elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing with Qwen2-VL-2B:  85%|████████▌ | 850/1000 [1:14:07<14:23,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 850/1000: 74.1m elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing with Qwen2-VL-2B:  90%|█████████ | 900/1000 [1:18:21<08:24,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 900/1000: 78.4m elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing with Qwen2-VL-2B:  95%|█████████▌| 950/1000 [1:22:41<04:08,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 950/1000: 82.7m elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing with Qwen2-VL-2B: 100%|██████████| 1000/1000 [1:26:55<00:00,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 1000/1000: 86.9m elapsed\n",
      "\n",
      "Completed Qwen2-VL-2B:\n",
      "  Processed: 1000/1000 images\n",
      "  Successful: 1000/1000\n",
      "  Time: 86.9 minutes\n",
      "  Avg per image: 5.2s\n",
      "  Saved to: results/results_Qwen2-VL-2B.csv\n",
      "\n",
      "================================================================================\n",
      "PROCESSING COMPLETE\n",
      "================================================================================\n",
      "Total results: 1000\n",
      "Saved to: results/all_models_comparison.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "section-analysis",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-analysis",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T01:45:55.262037Z",
     "start_time": "2025-11-18T01:45:55.246525Z"
    }
   },
   "source": "combined_path = os.path.join(RESULTS_DIR, 'all_models_comparison.csv')\nif os.path.exists(combined_path):\n    results_df = pd.read_csv(combined_path)\n    \n    print(f\"\\n{'='*80}\")\n    print(\"RESULTS SUMMARY\")\n    print(f\"{'='*80}\")\n    \n    for model_name in results_df['model_name'].unique():\n        model_data = results_df[results_df['model_name'] == model_name]\n        caption_lengths = model_data['caption'].str.len()\n        \n        print(f\"\\n{model_name}:\")\n        print(f\"  Total images: {len(model_data)}\")\n        print(f\"  Successful: {len([c for c in model_data['caption'] if not c.startswith('Error')])}\")\n        print(f\"  Avg caption length: {caption_lengths.mean():.0f} characters\")\n        print(f\"  Caption length range: {caption_lengths.min()}-{caption_lengths.max()}\")\n        print(f\"  Avg inference time: {model_data['processing_time_sec'].mean():.2f}s per image\")\n    \n    # Also save as JSON for flexibility\n    json_path = os.path.join(RESULTS_DIR, 'all_models_comparison.json')\n    results_df.to_json(json_path, orient='records', indent=2)\n    print(f\"\\n{'='*80}\")\n    print(f\"Results exported to:\")\n    print(f\"  CSV: {combined_path}\")\n    print(f\"  JSON: {json_path}\")\n    print(f\"{'='*80}\")\nelse:\n    print(\"No results file found.\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Qwen2-VL-2B:\n",
      "  Total images: 1000\n",
      "  Successful: 1000\n",
      "  Avg caption length: 1082 characters\n",
      "  Caption length range: 437-1424\n",
      "  Avg inference time: 5.21s per image\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "section-old-code",
   "metadata": {},
   "source": [
    "## OLD CODE - Kept for Reference (Commented Out)"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-old-code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T01:45:55.298499Z",
     "start_time": "2025-11-18T01:45:55.295864Z"
    }
   },
   "source": [
    "# ============================================\n",
    "# Old Gemini API approach - NO LONGER USED\n",
    "# ============================================\n",
    "# Switched to using open-source models instead\n",
    "# Benefits: No API costs, no rate limiting, full local control\n",
    "\n",
    "# import google.generativeai as genai\n",
    "#\n",
    "# GEMINI_API_KEY = \"your_api_key_here\"\n",
    "# genai.configure(api_key=GEMINI_API_KEY)\n",
    "#\n",
    "# def generate_gemini_caption(image):\n",
    "#     try:\n",
    "#         gemini_model = genai.GenerativeModel('models/gemini-2.5-pro')\n",
    "#         response = gemini_model.generate_content([\n",
    "#             \"Describe this image in detail.\",\n",
    "#             image\n",
    "#         ])\n",
    "#         return response.text\n",
    "#     except Exception as e:\n",
    "#         return f\"Error: {str(e)}\"\n",
    "#\n",
    "# ============================================\n",
    "# Old manual patch extraction - NOT NEEDED\n",
    "# ============================================\n",
    "# Modern VLMs handle this internally\n",
    "#\n",
    "# def extract_patches(image, patch_size=224, stride=112):\n",
    "#     patches = []\n",
    "#     width, height = image.size\n",
    "#     for y in range(0, height - patch_size + 1, stride):\n",
    "#         for x in range(0, width - patch_size + 1, stride):\n",
    "#             patch = image.crop((x, y, x + patch_size, y + patch_size))\n",
    "#             patches.append(patch)\n",
    "#     return patches\n",
    "\n",
    "print(\"Old code is kept here for reference only\")\n",
    "print(\"We are using open-source VLM models for better control and no API costs\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old code is kept here for reference only\n",
      "We are using open-source VLM models for better control and no API costs\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
