# VLM Zero-Shot Evaluation Framework for Radiology

This is a modular framework for evaluating Vision-Language Models (VLMs) on radiology images using zero-shot inference.

## Features

- **4 Small VLM Models** optimized for RTX 5080 (16GB VRAM):
  - Qwen2-VL-2B (6 GB)
  - Phi-3-Vision (10 GB)
  - InternVL2-2B (6 GB)
  - SmolVLM2-Instruct (5.2 GB)

- **Comprehensive Metrics**:
  - BLEU (1-4)
  - ROUGE (1, 2, L)
  - METEOR
  - ChrF
  - BERTScore (Precision, Recall, F1)
  - Perplexity

- **Visualizations**:
  - Histograms of metric distributions
  - Box plots comparing models
  - Heatmaps for model comparison
  - Metric correlation analysis
  - Performance summary charts

## Installation

```bash
cd /home/vortex/CSE\ 468\ AFE/Project/Evaluation/codes
pip install -r requirements.txt
```

## Dataset Structure

The framework expects:
- **Ground truth**: `/Evaluation/Reason-datasets/data.jsonl`
- **Images**: `/Evaluation/Reason-datasets/images/`

Format of `data.jsonl`:
```json
{
  "id": "PMC_00000",
  "image_path": "images/PMC_00000.jpg",
  "caption": "Ground truth caption",
  "reasoning": "...",
  "metadata": {...}
}
```

## Usage

### Quick Start (Test with 10 images, one model)

```bash
python run_evaluation.py --num-images 10 --models Qwen2-VL-2B
```

### Full Evaluation (500 images, all models)

```bash
python run_evaluation.py --num-images 500
```

### Evaluate specific models

```bash
python run_evaluation.py --num-images 500 --models Qwen2-VL-2B,Phi3-Vision
```

## Configuration

Edit `config.py` to customize:

```python
class EvaluationConfig:
    NUM_IMAGES = 500  # Number of images to process
    CHECKPOINT_INTERVAL = 50  # Save checkpoint every N images
    ZERO_SHOT_PROMPT = "..."  # Customize prompt for radiology
```

## Module Structure

```
codes/
├── config.py                 # Configuration settings
├── data_loader.py            # Load ground truth and images
├── vlm_models.py            # VLM model wrappers
├── metrics.py               # Evaluation metrics
├── visualizations.py        # Visualization generation
├── run_evaluation.py        # Main evaluation script
├── requirements.txt         # Python dependencies
└── README.md               # This file
```

## Output Structure

Results are saved in `/Evaluation/results/`:

```
results/
├── model_outputs/
│   ├── outputs_Qwen2-VL-2B.csv           # Generated captions
│   ├── detailed_results_Qwen2-VL-2B.csv  # With per-sample metrics
│   └── checkpoint_*.csv                   # Checkpoints
├── metrics/
│   └── evaluation_summary.csv            # Summary of all models
└── visualizations/
    ├── histogram_*.png                   # Metric distributions
    ├── boxplot_metrics_comparison.png    # Model comparison
    ├── heatmap_*.png                     # Heatmaps
    └── bar_performance_summary.png       # Summary chart
```

## Output CSV Format

### Model Outputs (`outputs_<model>.csv`)

| Column | Description |
|--------|-------------|
| image_id | Image identifier (e.g., PMC_00000) |
| image_path | Full path to image file |
| generated_caption | Caption generated by the model |
| ground_truth | Original ground truth caption |
| processing_time_sec | Time taken to generate caption |
| timestamp | ISO timestamp of generation |

### Detailed Results (`detailed_results_<model>.csv`)

Same as above, plus per-sample metrics:
- BLEU-4
- ROUGE-1
- ROUGE-L

### Evaluation Summary (`evaluation_summary.csv`)

| Column | Description |
|--------|-------------|
| model | Model name |
| BLEU-1/2/3/4 | BLEU scores |
| ROUGE-1/2/L | ROUGE scores |
| METEOR | METEOR score |
| ChrF | ChrF score |
| BERTScore-P/R/F1 | BERTScore metrics |
| Perplexity | Perplexity score |
| avg_processing_time | Average time per image |
| total_processing_time | Total processing time |

## Testing Individual Modules

Each module can be tested independently:

```bash
# Test configuration
python config.py

# Test data loader
python data_loader.py

# Test VLM models (downloads model weights)
python vlm_models.py

# Test metrics (may take time for initialization)
python metrics.py

# Test visualizations
python visualizations.py
```

## Memory Management

The framework automatically:
- Loads models in FP16 for memory efficiency
- Unloads models after processing
- Clears CUDA cache between models
- Saves checkpoints every 50 images

## Troubleshooting

### CUDA Out of Memory
- Reduce `NUM_IMAGES` in config
- Disable larger models (Phi3-Vision uses ~10GB)
- Ensure no other processes are using GPU

### Missing Dependencies
```bash
pip install -r requirements.txt
```

### Model Download Issues
- First run downloads models (~2-5 GB each)
- Requires stable internet connection
- Models cache in `~/.cache/huggingface/`

## Citation

If you use this framework, please cite the respective model papers:
- Qwen2-VL: [Paper](https://arxiv.org/abs/2409.12191)
- Phi-3-Vision: [Paper](https://arxiv.org/abs/2404.14219)
- InternVL2: [Paper](https://arxiv.org/abs/2404.16821)
- SmolVLM2: [Blog](https://huggingface.co/blog/smolvlm)
