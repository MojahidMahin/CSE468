# Metrics Analysis - Current Results

## Your Current Scores

| Metric | Your Score | Expected Range (Medical) | Status |
|--------|-----------|-------------------------|--------|
| **BLEU-4** | 0.0042 | 0.10 - 0.30 | ‚ùå Very Low (41x below minimum) |
| **BLEU-1** | 0.0434 | 0.30 - 0.50 | ‚ùå Very Low (7x below) |
| **ROUGE-1** | 0.0804 | 0.30 - 0.50 | ‚ùå Very Low (4x below) |
| **ROUGE-L** | 0.0650 | 0.20 - 0.40 | ‚ùå Very Low (3x below) |
| **METEOR** | 0.1260 | 0.20 - 0.30 | ‚ùå Low (needs 2x improvement) |
| **ChrF** | 0.1630 | 0.30 - 0.50 | ‚ùå Low (needs 2-3x improvement) |
| **BERTScore-F1** | -0.0712 | 0.75 - 0.90 | ‚ùå‚ùå NEGATIVE (anti-correlated!) |
| **Perplexity** | 9.23 | 5 - 15 | ‚úÖ OK (lower is better) |

## What This Means

### üö® Critical Issues

1. **BLEU-4 (0.0042)** - Catastrophically low
   - Measures n-gram overlap (4 consecutive words)
   - Score of 0.0042 means almost NO matching 4-word phrases
   - **Cause**: Long verbose outputs have few exact matches with short ground truth

2. **BERTScore-F1 (-0.0712)** - NEGATIVE score
   - Measures semantic similarity using contextual embeddings
   - Negative score means generated text is **semantically opposite** to ground truth
   - **Cause**: Generated text discusses "normal anatomy" while ground truth discusses "abnormalities"

3. **ROUGE scores (0.06-0.08)** - Very low
   - Measures recall of important words/phrases
   - Low scores mean missing key clinical terms
   - **Cause**: Verbose explanations dilute the key findings

## Why So Low?

### Problem Visualization

**Ground Truth** (72 chars):
```
"Chest X-ray shows fine bilateral reticulo-interstitial infiltrates"
```

**Your Model Output** (~500+ chars):
```
"This medical image is a chest X-ray, which is a type of radiographic
imaging used to visualize the structures within the chest. The X-ray is
taken from the front (anteroposterior [AP] view) and is typically used
to diagnose conditions such as pneumonia, lung cancer...

### Observations:
1. **Lungs**: The lungs appear clear and symmetric...
2. **Heart**: The heart is visible in the center..."
```

### The Math of Low Scores

**BLEU-4 Calculation Example:**
- Ground truth: "bilateral reticulo-interstitial infiltrates"
- Generated: "lungs appear clear and symmetric"
- Matching 4-grams: **0** (ZERO overlap!)
- Result: Score ‚âà 0

**Length Penalty:**
- Ground truth: ~15 words
- Generated: ~100+ words
- BLEU/ROUGE heavily penalize length mismatch
- Your output is 6-7x longer ‚Üí massive penalty

## Expected Scores with Fixed Prompt

With the recommended concise prompt, typical medical captioning scores:

| Metric | Current | Expected with Fix | Improvement |
|--------|---------|-------------------|-------------|
| BLEU-4 | 0.0042 | 0.08 - 0.15 | **20-35x better** |
| ROUGE-1 | 0.0804 | 0.25 - 0.40 | **3-5x better** |
| ROUGE-L | 0.0650 | 0.20 - 0.35 | **3-5x better** |
| METEOR | 0.1260 | 0.22 - 0.30 | **2x better** |
| BERTScore-F1 | -0.0712 | 0.75 - 0.85 | **10x+ better** |

## Benchmark Comparison

### General Image Captioning (COCO dataset)
- BLEU-4: 0.30 - 0.40
- ROUGE-L: 0.50 - 0.60
- CIDEr: 1.0 - 1.3

### Medical Image Captioning (typical)
- BLEU-4: 0.10 - 0.30 (harder than general)
- ROUGE-L: 0.20 - 0.40
- BERTScore: 0.75 - 0.90

### Your Current Scores
- BLEU-4: 0.0042 ‚Üê **100x worse than medical benchmarks!**
- ROUGE-L: 0.0650 ‚Üê **3-6x worse**
- BERTScore: -0.0712 ‚Üê **Negative (worst possible)**

## Root Cause Analysis

### Issue #1: Style Mismatch
- **Ground Truth**: Clinical, concise findings
- **Generated**: Educational, verbose explanations
- **Impact**: No semantic overlap

### Issue #2: Length Mismatch
- **Ground Truth**: 50-150 characters
- **Generated**: 400-600 characters
- **Impact**: Heavy BLEU/ROUGE penalties

### Issue #3: Content Focus
- **Ground Truth**: Abnormalities, specific findings
- **Generated**: Normal anatomy, general observations
- **Impact**: Missing key clinical terms

### Issue #4: Prompt Design
```python
# Current (BAD)
"Describe this medical image in detail..."
          ‚Üë‚Üë‚Üë
      Causes verbosity!

# Recommended (GOOD)
"Provide a concise radiology report finding... in 1-2 sentences."
          ‚Üë‚Üë‚Üë              ‚Üë‚Üë‚Üë
    Sets clinical tone   Enforces brevity
```

## Action Items

### ‚úÖ Immediate Fix (Will Improve Scores 3-10x)

1. **Update prompt in config.py:**
```python
ZERO_SHOT_PROMPT = (
    "Provide a concise radiology report finding for this medical image. "
    "Describe only the key clinical observations in 1-2 sentences."
)
```

2. **Test on 10 images:**
```bash
python run_evaluation.py --num-images 10 --models Qwen2-VL-2B
```

3. **Expected improvement:**
   - BLEU-4: 0.004 ‚Üí 0.08-0.12
   - ROUGE-L: 0.065 ‚Üí 0.20-0.30
   - BERTScore: -0.07 ‚Üí 0.75-0.82

### üìä Compare Before/After

After re-running with new prompt, compare:
```bash
# Old results (current)
results/metrics/evaluation_summary.csv

# New results (after prompt fix)
results/metrics/evaluation_summary_new.csv
```

## Bottom Line

**Your current scores are 3-100x lower than expected for medical image captioning.**

The primary reason: **Prompt mismatch causing verbose, off-topic outputs**

**Estimated time to fix:**
- Edit config.py: 1 minute
- Test 10 images: 2 minutes
- See dramatic improvement in scores

**This is an easy fix with massive impact!** üéØ
